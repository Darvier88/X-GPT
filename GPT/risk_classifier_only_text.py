"""Clasificador de Riesgos para Tweets (B3) - SIN ANÃLISIS DE MEDIA
- AnÃ¡lisis SOLO de texto
- Prompts simplificados para velocidad
- Modelo mÃ¡s rÃ¡pido: gpt-4o-mini
- Token Budget Tracker mejorado
"""

import time
import json
import re
from typing import Optional, List, Dict, Any, Tuple
from collections import deque
from openai import OpenAI, APIError, RateLimitError, APITimeoutError
from pathlib import Path
import sys
sys.path.insert(0, str(Path(__file__).resolve().parents[1]))
from config import get_openai_api_key

# ========================================================================
# POLÃTICA v1.0 - VERSIÃ“N COMPACTA (mismo significado)
# ========================================================================

POLICY_COMPACT = {
    "version": "v1.0",
    "categories": {
        "toxic": "Insultos, lenguaje denigrante a personas/grupos",
        "hate": "Ataques por identidad protegida (raza, religiÃ³n, gÃ©nero, etc.)",
        "political_sensitivity": "Extremismo, conspiraciÃ³n, desinformaciÃ³n polÃ­tica",
        "nsfw": "Contenido sexual explÃ­cito o pornogrÃ¡fico",
        "bullying": "IntimidaciÃ³n, acoso persistente, doxxing",
        "violence": "Amenazas, incitaciÃ³n a violencia, autolesiones",
        "legal_privacy": "DifamaciÃ³n, PII sensible (tel, direcciÃ³n, doc)"
    },
    "levels": {
        "no": "Sin riesgo",
        "low": "Riesgo menor/incierto",
        "mid": "Riesgo claro, daÃ±o reputacional posible",
        "high": "Riesgo severo, violaciÃ³n probable"
    },
    "rules": {
        "escalate": ["hate/violence â†’ high", "PII sensible â†’ high"],
        "deescalate": ["Cita/sarcasmo evidente â†’ bajar"]
    }
}

# ConfiguraciÃ³n optimizada
MAX_RETRIES = 2
REQUEST_TIMEOUT = 20
TIMEOUT_PER_TWEET = 40
CIRCUIT_THRESHOLD = 10
CIRCUIT_COOLDOWN = 120
DELAY_BETWEEN_TWEETS = 0.8
DELAY_AFTER_RATE_LIMIT = 10

ERROR_CODES = {
    'timeout': 'timeout',
    'rate_limit': 'rate_limit',
    'api_error': 'api_error',
    'auth_error': 'auth_error',
    'content_filtered': 'content_filtered',
    'circuit_open': 'circuit_open',
    'tweet_timeout': 'tweet_timeout',
    'unknown': 'unknown'
}


# ========================================================================
# TOKEN BUDGET TRACKER
# ========================================================================

class TokenBudgetTracker:
    """Rastrea el uso de tokens para evitar rate limits proactivamente."""
    
    def __init__(self, tokens_per_minute: int = 140000):  # 70% del lÃ­mite (mÃ¡s conservador)
        self.tokens_per_minute = tokens_per_minute
        self.window_seconds = 60
        self.requests = deque()
        
    def get_current_usage(self) -> int:
        now = time.time()
        cutoff = now - self.window_seconds
        while self.requests and self.requests[0][0] < cutoff:
            self.requests.popleft()
        return sum(tokens for _, tokens in self.requests)
    
    def can_make_request(self, estimated_tokens: int) -> bool:
        current = self.get_current_usage()
        return (current + estimated_tokens) <= self.tokens_per_minute
    
    def wait_for_budget(self, estimated_tokens: int) -> float:
        if self.can_make_request(estimated_tokens):
            return 0.0
        
        now = time.time()
        while self.requests:
            oldest_time, oldest_tokens = self.requests[0]
            if oldest_time < (now - self.window_seconds):
                self.requests.popleft()
                continue
            
            wait_time = (oldest_time + self.window_seconds) - now + 1.5
            return max(0.0, wait_time)
        
        return 1.0
    
    def record_request(self, tokens_used: int):
        self.requests.append((time.time(), tokens_used))
    
    def get_usage_percentage(self) -> float:
        return (self.get_current_usage() / self.tokens_per_minute) * 100


token_tracker = TokenBudgetTracker(tokens_per_minute=140000)  # 70% del lÃ­mite


# ========================================================================
# ESTIMACIÃ“N DE TOKENS
# ========================================================================

def estimate_tokens(text: str) -> int:
    """Estima tokens solo para texto."""
    base_tokens = 600  # Prompt simplificado
    text_tokens = len(text) // 3
    response_tokens = 250
    
    return base_tokens + text_tokens + response_tokens


# ========================================================================
# CIRCUIT BREAKER
# ========================================================================

class CircuitBreaker:
    def __init__(self, threshold: int = CIRCUIT_THRESHOLD, cooldown: int = CIRCUIT_COOLDOWN):
        self.threshold = threshold
        self.cooldown = cooldown
        self.failures = 0
        self.opened_at: Optional[float] = None

    def record_success(self):
        self.failures = 0
        self.opened_at = None

    def record_failure(self):
        self.failures += 1
        if self.failures >= self.threshold and self.opened_at is None:
            self.opened_at = time.monotonic()

    def is_open(self) -> bool:
        if self.opened_at is None:
            return False
        elapsed = time.monotonic() - self.opened_at
        if elapsed >= self.cooldown:
            self.failures = 0
            self.opened_at = None
            return False
        return True


circuit_with_policy = CircuitBreaker()


# ========================================================================
# PROMPT SIMPLIFICADO (SOLO TEXTO)
# ========================================================================

def build_text_prompt(tweet_text: str) -> str:
    """
    Prompt compacto para analizar SOLO texto.
    VersiÃ³n simplificada pero mantiene significado de la polÃ­tica.
    """
    
    categories = "\n".join([f"- {k}: {v}" for k, v in POLICY_COMPACT["categories"].items()])

    prompt = f"""Clasifica riesgo segÃºn PolÃ­tica v1.0 (compacta).

CATEGORÃAS:
{categories}

NIVELES: low (menor), mid (claro), high (severo)

REGLAS:
- hate/violence â†’ high
- Cita/sarcasmo evidente â†’ bajar nivel
- PII (tel/direcciÃ³n) â†’ high

EJEMPLOS:

Tweet: "People who believe X are idiots."
{{"labels":["toxic"],"risk_level":"mid","rationale":"Insulto genÃ©rico sin slur","spans":[{{"text":"idiots","start":24,"end":30,"label":"toxic"}}],"confidence":0.78}}

Tweet: "Estos inmigrantes son basura"
{{"labels":["hate","toxic"],"risk_level":"high","rationale":"DeshumanizaciÃ³n de grupo protegido","spans":[{{"text":"Estos inmigrantes son basura","start":0,"end":28,"label":"hate"}}],"confidence":0.89}}

TWEET: "{tweet_text}"

Responde SOLO JSON:
{{"labels":[...],"risk_level":"low|mid|high","rationale":"breve","spans":[...],"confidence":0.0-1.0}}"""
    
    return prompt


# ========================================================================
# CLASIFICACIÃ“N (SOLO TEXTO)
# ========================================================================

def classify_risk_text_only(tweet_text: str) -> Dict[str, Any]:
    """
    Analiza SOLO texto (sin media).
    MÃ¡s rÃ¡pido y eficiente.
    """
    start_time = time.monotonic()

    if circuit_with_policy.is_open():
        return {
            "error_code": ERROR_CODES['circuit_open'],
            "error": "Circuit breaker abierto"
        }

    # Estimar tokens
    estimated_tokens = estimate_tokens(tweet_text)
    
    # THROTTLING
    wait_time = token_tracker.wait_for_budget(estimated_tokens)
    if wait_time > 0:
        print(f" â³{wait_time:.1f}s", end="", flush=True)
        time.sleep(wait_time)

    try:
        client = OpenAI(api_key=get_openai_api_key())
    except Exception as e:
        circuit_with_policy.record_failure()
        return {"error_code": ERROR_CODES['auth_error'], "error": str(e)}

    prompt = build_text_prompt(tweet_text)
    
    messages = [
        {"role": "system", "content": "Clasificador de riesgos. Responde SOLO JSON vÃ¡lido."},
        {"role": "user", "content": prompt}
    ]

    attempts_allowed = MAX_RETRIES + 1

    for attempt in range(1, attempts_allowed + 1):
        if time.monotonic() - start_time >= TIMEOUT_PER_TWEET:
            circuit_with_policy.record_failure()
            return {"error_code": ERROR_CODES['tweet_timeout'], "error": "Timeout", "attempt": attempt}

        try:
            response = client.chat.completions.create(
                model="gpt-4o-mini",  # Modelo rÃ¡pido y econÃ³mico
                messages=messages,
                temperature=0.2,  # MÃ¡s determinÃ­stico
                max_tokens=400,  # Reducido para respuestas mÃ¡s rÃ¡pidas
                timeout=REQUEST_TIMEOUT
            )
            
            # Registrar tokens
            tokens_used = response.usage.total_tokens if hasattr(response, 'usage') else estimated_tokens
            token_tracker.record_request(tokens_used)

            if not getattr(response, "choices", None):
                print(f" R{attempt}", end="", flush=True)
                time.sleep(0.3)
                continue

            choice = response.choices[0]
            finish_reason = getattr(choice, "finish_reason", "unknown")
            content = getattr(choice.message, "content", "").strip()

            if finish_reason in ("content_filter", "content_filtered"):
                circuit_with_policy.record_failure()
                return {"error_code": ERROR_CODES['content_filtered'], "error": "Filtrado", "attempt": attempt}

            if not content:
                print(f" E{attempt}", end="", flush=True)
                time.sleep(0.3)
                continue

            # Parsear JSON
            try:
                json_match = re.search(r'\{[\s\S]*\}', content)
                data = json.loads(json_match.group(0) if json_match else content)
            except Exception as e:
                if attempt >= attempts_allowed:
                    return {
                        "labels": [], "risk_level": "no", "rationale": "Error parseando",
                        "spans": [], "attempt": attempt, "parse_error": str(e)
                    }
                time.sleep(0.3)
                continue

            # Validar
            labels = [l for l in data.get("labels", []) if l in POLICY_COMPACT["categories"]]
            # permitir 'no' como valor vÃ¡lido y usarlo por defecto
            risk_level = data.get("risk_level", "no")
            if risk_level not in ["no", "low", "mid", "high"]:
                risk_level = "no"
            
            rationale = data.get("rationale", "")
            confidence = max(0.0, min(1.0, float(data.get("confidence", 0.5))))
            spans = [s for s in data.get("spans", []) if isinstance(s, dict) and "text" in s]
            
            if labels and not spans:
                spans = extract_spans_fallback(tweet_text, labels)

            if not labels:
                # Si no hay etiquetas explÃ­citas, marcar como 'no' (sin riesgo)
                risk_level = "no"

            # Aplicar reglas de polÃ­tica
            original_level = risk_level
            policy_applied = None
            risk_level, policy_applied = apply_policy_rules(labels, risk_level, tweet_text)

            circuit_with_policy.record_success()
            
            result = {
                "labels": labels,
                "risk_level": risk_level,
                "rationale": rationale,
                "spans": spans,
                "confidence": confidence,
                "attempt": attempt,
                "finish_reason": finish_reason,
                "policy_applied": policy_applied
            }
            
            if original_level != risk_level:
                result["original_risk_level"] = original_level
            
            return result

        except RateLimitError as e:
            circuit_with_policy.record_failure()
            print(f" RL{attempt}", end="", flush=True)
            
            # Esperas PROGRESIVAS: cada intento espera mÃ¡s
            error_msg = str(e)
            base_wait = 5.0
            
            match = re.search(r'try again in (\d+)ms', error_msg)
            if match:
                ms_to_wait = float(match.group(1))
                base_wait = (ms_to_wait / 1000.0) + 3.0
            
            # Espera progresiva: intento 1=5s, intento 2=10s, intento 3=15s
            wait_time = max(base_wait * attempt, 5.0)
            
            print(f"({wait_time:.1f}s)", end="", flush=True)
            time.sleep(wait_time)
            
            if attempt >= attempts_allowed:
                return {"error_code": ERROR_CODES['rate_limit'], "error": "Rate limit", "attempt": attempt}
            continue

        except (APITimeoutError, APIError) as e:
            circuit_with_policy.record_failure()
            print(f" E{attempt}", end="", flush=True)
            
            if isinstance(e, APITimeoutError) and time.monotonic() - start_time >= TIMEOUT_PER_TWEET:
                return {"error_code": ERROR_CODES['timeout'], "error": "Timeout", "attempt": attempt}
            
            time.sleep(0.5)
            if attempt >= attempts_allowed:
                return {"error_code": ERROR_CODES['api_error'], "error": str(e), "attempt": attempt}
            continue

        except Exception as e:
            circuit_with_policy.record_failure()
            print(f" X{attempt}", end="", flush=True)
            time.sleep(0.5)
            if attempt >= attempts_allowed:
                return {"error_code": ERROR_CODES['unknown'], "error": str(e), "attempt": attempt}
            continue

    return {"error_code": ERROR_CODES['unknown'], "error": "Fallos mÃºltiples", "attempts": attempts_allowed}


# ========================================================================
# REGLAS DE POLÃTICA
# ========================================================================

def apply_policy_rules(labels: List[str], risk_level: str, text: str) -> Tuple[str, str]:
    """Aplica reglas compactas de polÃ­tica."""
    reasoning = []
    
    # Si hay hate/violence escalar incluso desde 'no' o 'low' o 'mid'
    if ("hate" in labels or "violence" in labels) and risk_level in ("no", "low", "mid"):
        risk_level = "high"
        reasoning.append("hate/violenceâ†’high")
    
    serious_labels = {"hate", "violence", "legal_privacy"}
    if len([l for l in labels if l in serious_labels]) > 1:
        risk_level = "high"
        reasoning.append("mÃºltiples seriosâ†’high")
    
    if ("RT:" in text or "Cita:" in text or "ironÃ­a" in text.lower() or "sarcasmo" in text.lower()):
        if risk_level == "high":
            risk_level = "mid"
            reasoning.append("cita/sarcasmoâ†’mid")
    
    return risk_level, " | ".join(reasoning) if reasoning else "sin cambios"


# ========================================================================
# EXTRACCIÃ“N DE SPANS (FALLBACK)
# ========================================================================

def extract_spans_fallback(tweet_text: str, labels: List[str]) -> List[Dict[str, Any]]:
    """ExtracciÃ³n heurÃ­stica bÃ¡sica."""
    spans = []
    patterns = {
        'toxic': [r'\b(idiota|estÃºpido|imbÃ©cil|pendejo|cabrÃ³n|mierda|basura|fuck|shit|bitch)\b'],
        'violence': [r'\b(matar|golpear|partir|romper|atacar)\b.*\b(cara|cabeza)\b'],
        'hate': [r'\b(nazi|fascista|terrorista)\b'],
        'bullying': [r'\b(acoso|hostigar|te voy a encontrar)\b'],
        'legal_privacy': [r'\b\d{3}[-.\s]?\d{3}[-.\s]?\d{4}\b']
    }
    
    for label in labels:
        if label in patterns:
            for pattern in patterns[label]:
                for match in re.finditer(pattern, tweet_text, re.IGNORECASE):
                    spans.append({
                        'text': match.group(0),
                        'start': match.start(),
                        'end': match.end(),
                        'label': label
                    })
    
    return spans


# ========================================================================
# CARGA DE TWEETS
# ========================================================================

def load_tweets_from_json(json_path: str) -> List[Dict[str, Any]]:
    """Carga tweets desde JSON."""
    p = Path(json_path)
    if not p.exists():
        raise FileNotFoundError(f"No existe: {p}")
    with p.open("r", encoding="utf-8") as f:
        data = json.load(f)
    
    if "tweets" in data:
        return data.get("tweets", [])
    return data


BATCH_SIZE = 50


# ========================================================================
# MAIN OPTIMIZADO
# ========================================================================

if __name__ == "__main__":
    print("\n" + "="*70)
    print("ğŸ›¡ï¸  CLASIFICADOR OPTIMIZADO v1.0 - Solo Texto")
    print("   âš¡ Prompts compactos | Modelo rÃ¡pido | Sin anÃ¡lisis de media")
    print("="*70)

    default_json = Path(__file__).resolve().parents[1] / "tweets_TheDarkraimola_20251120_180501.json"
    try:
        result_data = load_tweets_from_json(str(default_json))
        
        if isinstance(result_data, dict) and "tweets" in result_data:
            tweets_data = result_data["tweets"]
        elif isinstance(result_data, list):
            tweets_data = result_data
        else:
            tweets_data = []
        
        test_tweets = []
        for t in tweets_data:
            if isinstance(t, dict):
                tweet_text = t.get("text", "")
                if tweet_text.strip():
                    test_tweets.append(tweet_text)
        
        print(f"ğŸ“¥ {len(test_tweets)} tweets cargados")
    except Exception as e:
        print(f"âš ï¸ Error: {e}")
        test_tweets = [
            "Â¡Hermoso dÃ­a! ğŸŒ",
            "Este polÃ­tico es un idiota corrupto",
        ]

    total = len(test_tweets)
    print(f"\nğŸ“Š Analizando {total} tweets...\n")

    results = []
    stats = {
        "risk_distribution": {"no": 0, "low": 0, "mid": 0, "high": 0},
        "label_counts": {},
        "errors": 0,
        "times": [],
        "throttle_waits": 0
    }
    
    program_start = time.monotonic()
    estimated_time_str = None

    total_batches = (total + BATCH_SIZE - 1) // BATCH_SIZE
    
    for batch_idx, batch_start in enumerate(range(0, total, BATCH_SIZE), start=1):
        batch = test_tweets[batch_start:batch_start + BATCH_SIZE]
        print(f"\n{'='*60}")
        print(f"ğŸ” Lote {batch_idx}/{total_batches} â€” {batch_start+1}-{batch_start+len(batch)}")
        print(f"{'='*60}\n")

        for idx, tweet_text in enumerate(batch, start=batch_start+1):
            # Mostrar progreso compacto
            usage_pct = token_tracker.get_usage_percentage()
            print(f"ğŸ¦ {idx:3d}/{total} [{usage_pct:3.0f}%] ", end="", flush=True)
            
            # Throttling preventivo MÃS AGRESIVO
            if usage_pct > 60:  # Bajado de 70% a 60%
                wait_time = 8.0  # Aumentado de 5s a 8s
                print(f"âš ï¸ {wait_time}s", end="", flush=True)
                time.sleep(wait_time)
                stats["throttle_waits"] += 1

            start = time.monotonic()
            result = classify_risk_text_only(tweet_text)
            elapsed = time.monotonic() - start
            stats["times"].append(elapsed)
            
            if "error_code" not in result:
                level = result.get("risk_level", "low")
                stats["risk_distribution"][level] += 1
                for label in result.get("labels", []):
                    stats["label_counts"][label] = stats["label_counts"].get(label, 0) + 1
            else:
                stats["errors"] += 1
            
            result["tweet_id"] = idx
            result["text"] = tweet_text
            results.append(result)
            
            # Mostrar resultado compacto
            risk_str = result.get('risk_level', 'ERR')
            labels_str = ",".join(result.get('labels', []))[:20]
            print(f" â†’ {risk_str:4s} {labels_str:20s} ({elapsed:.1f}s)")
            
            # EstimaciÃ³n mejorada despuÃ©s de 10 tweets (mÃ¡s representativo)
            if len(stats["times"]) == 10 and not estimated_time_str:
                # Calcular tiempo real total incluido delays
                elapsed_so_far = time.monotonic() - program_start
                avg_time_per_tweet = elapsed_so_far / 10  # Incluye TODO (anÃ¡lisis + delays + throttles)
                
                # Proyectar para tweets restantes
                remaining_tweets = total - 10
                est_remaining = avg_time_per_tweet * remaining_tweets
                est_total = elapsed_so_far + est_remaining
                
                est_hours = int(est_total // 3600)
                est_min = int((est_total % 3600) // 60)
                est_sec = int(est_total % 60)
                
                if est_hours > 0:
                    estimated_time_str = f"â‰ˆ{est_hours}h{est_min}m"
                elif est_min > 0:
                    estimated_time_str = f"â‰ˆ{est_min}m{est_sec}s"
                else:
                    estimated_time_str = f"â‰ˆ{est_sec}s"
                
                print(f"\n{'='*60}")
                print(f"â±ï¸  TIEMPO ESTIMADO TOTAL: {estimated_time_str}")
                print(f"   (Basado en {elapsed_so_far:.1f}s para primeros 10 tweets)")
                print(f"   Velocidad: {avg_time_per_tweet:.2f}s por tweet")
                print(f"{'='*60}\n")
                
                # Guardar estimaciÃ³n
                timing_file = Path("tiempo_estimado.json")
                timing_file.write_text(json.dumps({
                    "num_tweets": total,
                    "tweets_procesados": 10,
                    "tiempo_transcurrido": f"{int(elapsed_so_far)}s",
                    "tiempo_estimado_total": estimated_time_str,
                    "velocidad_promedio": f"{avg_time_per_tweet:.2f}s/tweet"
                }, ensure_ascii=False, indent=2), encoding="utf-8")

            # Delay adaptativo mÃ¡s conservador
            usage_pct = token_tracker.get_usage_percentage()
            if usage_pct > 55:  # Mayor a 55%
                time.sleep(DELAY_BETWEEN_TWEETS * 4.0)  # 3.2s
            elif usage_pct > 40:  # Mayor a 40%
                time.sleep(DELAY_BETWEEN_TWEETS * 2.5)  # 2.0s
            else:  # Menor a 40%
                time.sleep(DELAY_BETWEEN_TWEETS)  # 0.8s

    # Resumen final
    total_time = time.monotonic() - program_start
    total_min = int(total_time // 60)
    total_sec = int(total_time % 60)
    actual_time = f"{total_min}m{total_sec}s" if total_min else f"{total_sec}s"

    print("\n" + "="*70)
    print(f"ğŸ“ˆ RESUMEN - Tiempo: {actual_time}")
    print("="*70)

    successful = total - stats["errors"]
    print(f"\nâœ… Exitosos: {successful}/{total}")
    print(f"âŒ Errores: {stats['errors']}/{total}")
    print(f"â¸ï¸  Throttles: {stats['throttle_waits']}")
    
    if successful > 0:
        print(f"\nğŸ“Š DistribuciÃ³n:")
        for level in ["no", "low", "mid", "high"]:
            count = stats['risk_distribution'][level]
            pct = (count/successful*100)
            print(f"  {level:4s}: {count:3d} ({pct:5.1f}%)")
        
        print(f"\nğŸ·ï¸  Labels:")
        for label, count in sorted(stats["label_counts"].items(), key=lambda x: x[1], reverse=True)[:10]:
            print(f"  {label:20s}: {count:3d}")

    # Guardar resultados
    summary = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "tiempo_total": actual_time,
        "total_tweets": total,
        "exitosos": successful,
        "errores": stats["errors"],
        "distribucion": stats["risk_distribution"],
        "labels": stats["label_counts"]
    }

    Path("risk_summary_text_only.json").write_text(json.dumps(summary, ensure_ascii=False, indent=2), encoding="utf-8")
    Path("risk_detailed_text_only.json").write_text(json.dumps({"resultados": results}, ensure_ascii=False, indent=2), encoding="utf-8")
    
    print(f"\nğŸ’¾ Guardado: risk_summary_text_only.json")
    print(f"\nğŸ’¾ Guardado: risk_detailed_text_only.json")
    print("\n" + "="*70)
    print("âœ¨ Completado")
    print("="*70)